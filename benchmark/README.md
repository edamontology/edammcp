Given a list of “well annotated” biotools packages (example: metabolomics), find a way to auto-generate benchmark question-answer pairs to evaluate the quality of LLM mappings to EDAM terms.

Output: A CSV file with two columns, question and answer (self-contained). E.g.:

| Question | Answer |
| :---- | :---- |
| What operations does this tool perform? *xcms enables graphical inspection of spectral data, quantitative analysis without internal standards, quality assessment, alignment of retention times across samples, automated feature extraction, visualization of chromatographic traces, and removal of unwanted signals in mass spectrometry datasets.* | http://edamontology.org/operation\_3694; http://edamontology.org/operation\_3634; http://edamontology.org/operation\_2428; http://edamontology.org/operation\_3628; http://edamontology.org/operation\_3215; http://edamontology.org/operation\_3203; http://edamontology.org/operation\_3695 |

Question generation process: static and dynamic

* Questions are generated by a semi-deterministic, LLM-supported process that combines a static part with dynamic content generated from a variety of sources; the simplest ground truth for creating question-answer pairs is just the biotools JSON parsed by the LLM.  
* Slighly more involved: generate questions from README and biotools JSON

Prompts:

* *Generate a set of questions you could ask this json file to get the content of the individual fields, specifically the EDAM terms (operations, data, formats and topics) and store them in the qa.tsv file in a “Question” column.*  
* *Generate a paragraph per question from the readme.md and the biotools.json file which could be used to answer the respective questions specified in the qa.csv file and extend the question column with this by appending this text to the question. There is no need to include the source of the information in the question.*  
* *Using the qa.tsv file and the fields in the biotools.json file, generate an “Answer” column containing the EDAM ontology URIs to data, formats, topics and operations that answer the respective question.*

Potential information sources we could consume to have to create the benchmark questions with an LLM:

* Publication  
* [Readme.md](http://Readme.md) file  
* PyPI index page  
* Bioconductor index page, vignette  
* CRAN package DESCRIPTION file  
* pyproject.toml  
* nf-core metadata files  
* Code (from GitHub or by installing the module from pip)  
  * Docstrings  
  * Actual code (function signatures or methods)  
* bio.tools json